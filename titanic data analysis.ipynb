{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOs6jnkPBfn+4xlOg7jXq6O"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["!pip install pyspark\n","import pyspark.sql.functions as F\n","from pyspark.sql import SparkSession\n","\n","# Create a SparkSession\n","spark = SparkSession.builder.appName(\"TitanicAnalysis\").getOrCreate()\n","\n","# Load the CSV data using PySpark, indicating no header row\n","titanic_data = spark.read.csv(\"/content/TitanicData.csv\", header=False)\n","\n","# Assign descriptive column names based on your understanding of the data\n","titanic_data = titanic_data.withColumnRenamed(\"_c0\", \"Survived\") \\\n","                         .withColumnRenamed(\"_c1\", \"Pclass\") \\\n","                         .withColumnRenamed(\"_c2\", \"Name\") \\\n","                         .withColumnRenamed(\"_c3\", \"sex\") \\\n","                         .withColumnRenamed(\"_c4\", \"Age\") \\\n","                         .withColumnRenamed(\"_c5\", \"SibSp\") \\\n","                         .withColumnRenamed(\"_c6\", \"Parch\") \\\n","                         .withColumnRenamed(\"_c7\", \"Ticket\") \\\n","                         .withColumnRenamed(\"_c8\", \"Fare\") \\\n","                         .withColumnRenamed(\"_c9\", \"Cabin\") \\\n","                         .withColumnRenamed(\"_c10\", \"Embarked\")\n","\n","\n","# Extract relevant columns\n","feature_data = titanic_data.select(\"Survived\", \"sex\", \"Age\")\n","\n","# Split data based on survival\n","survived_passanger = feature_data.filter(feature_data.Survived == 1)\n","died_passanger = feature_data.filter(feature_data.Survived == 0)\n","\n","sur_avg_age = survived_passanger.groupBy(\"sex\").agg(F.avg(\"Age\").alias(\"avg_age\")) \\\n","                               .withColumn(\"category\", F.concat(F.lit(\"Survived_\"), F.col(\"sex\")))\n","died_avg_age = died_passanger.groupBy(\"sex\").agg(F.avg(\"Age\").alias(\"avg_age\")) \\\n","                             .withColumn(\"category\", F.concat(F.lit(\"Died_\"), F.col(\"sex\")))\n","\n","\n","# Merge data\n","merged_data = sur_avg_age.union(died_avg_age)\n","\n","# Store the report in Colab's temporary directory (adjust as needed)\n","merged_data.write.csv(\"/tmp/aaa_report.txt\", mode=\"overwrite\")\n","\n","# Load and display the report\n","report = spark.read.csv(\"/tmp/aaa_report.txt\", header=True)\n","report.show()\n","\n","\n"],"metadata":{"id":"TwDrEMI5uQaG","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1710048848120,"user_tz":-300,"elapsed":74872,"user":{"displayName":"Hamid Chaudhry","userId":"14244720525949144513"}},"outputId":"66f84ce3-86af-4058-e955-da52ed10271a"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pyspark\n","  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n","Building wheels for collected packages: pyspark\n","  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488493 sha256=6c8ca8db5f40ae1d6947c569018d05c52046a5b7e77bb5e2344c203466308412\n","  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n","Successfully built pyspark\n","Installing collected packages: pyspark\n","Successfully installed pyspark-3.5.1\n","+------+------------------+---------------+\n","|female| 28.84771573604061|Survived_female|\n","+------+------------------+---------------+\n","|  male|27.276021505376345|  Survived_male|\n","|  male|31.618055555555557|      Died_male|\n","+------+------------------+---------------+\n","\n"]}]},{"cell_type":"code","source":["feature_data = titanic_data.select('Survived', 'Pclass')\n","\n","# Split data into survived and died groups\n","died = feature_data.filter(F.col('Survived') == 0)\n","survived = feature_data.filter(F.col('Survived') == 1)\n","\n","# Group by Pclass and count survivals and deaths\n","died_pclass_group = died.groupBy('Pclass').count()\n","survived_pclass_group = survived.groupBy('Pclass').count()\n","\n","# Combine and format results\n","report_df = (\n","    died_pclass_group.withColumn('category', F.concat(F.lit('Died_Pclass_'), F.col('Pclass')))\n","      .union(survived_pclass_group.withColumn('category', F.concat(F.lit('Survived_Pclass_'), F.col('Pclass'))))\n","      .select('category', 'count')\n",")\n","\n","# Save as text file\n","report_df.write.csv('/titanic_analysis/Pclass_analysis.txt')\n","\n","# Optionally load and display the saved results\n","report_data = spark.read.csv('/titanic_analysis/Pclass_analysis.txt', header=True, inferSchema=True)\n","report_data.show()"],"metadata":{"id":"C6CKwzqyuzcM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1710048890678,"user_tz":-300,"elapsed":2931,"user":{"displayName":"Hamid Chaudhry","userId":"14244720525949144513"}},"outputId":"ca089a85-b1d9-4a26-8a0b-1af274fa5253"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["+-----------------+---+\n","|Survived_Pclass_3|119|\n","+-----------------+---+\n","|Survived_Pclass_1|136|\n","|Survived_Pclass_2| 87|\n","|    Died_Pclass_1| 80|\n","|    Died_Pclass_2| 97|\n","+-----------------+---+\n","\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"iVky-iS63ank"},"execution_count":null,"outputs":[]}]}